{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2pzvilVz88u"
      },
      "outputs": [],
      "source": [
        "# !pip install datasets\n",
        "# !pip install evaluate\n",
        "# !pip install rouge_score\n",
        "!pip install transformers datasets evaluate rouge_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import DataCollatorForSeq2Seq\n",
        "import numpy as np\n",
        "import evaluate\n",
        "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer"
      ],
      "metadata": {
        "id": "-wwckdNA0Gg4"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load Dataset**"
      ],
      "metadata": {
        "id": "h1kt1iz80St8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds_full = load_dataset('multi_news')\n",
        "ds_train = ds_full['train']\n",
        "ds_5000 = ds_train.shuffle(seed=42).select(range(5000))\n",
        "dataset = ds_5000.train_test_split(test_size=0.2)"
      ],
      "metadata": {
        "id": "jFKg_Qjw0SGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset.shape)"
      ],
      "metadata": {
        "id": "y6uuNuWX0WKz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f72c0ed5-d5cb-4b27-efaa-f7dc31c201e0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train': (4000, 2), 'test': (1000, 2)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preprocess Data**"
      ],
      "metadata": {
        "id": "PR8pUn230Ryo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "checkpoint = \"google-t5/t5-small\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "UtxtaEr8eIK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prefix = \"summarize: \"\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = [prefix + doc for doc in examples[\"document\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True)\n",
        "\n",
        "    # Tokenize the labels\n",
        "    labels = tokenizer(text_target=examples[\"summary\"], max_length=128, truncation=True)\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "# Assuming 'dataset' is already loaded and defined\n",
        "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "# Initialize the data collator\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)"
      ],
      "metadata": {
        "id": "UJOWki9FWU5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checkpoint = \"google-t5/t5-small\" # We need an encoder-decoder model since we're going text-text\n",
        "# tokenizer = AutoTokenizer.from_pretrained(checkpoint) # Use the right tokenizer\n",
        "# prefix = \"summarize: \" # This is a multipurpose model - we need to attach a task to tell it what we want"
      ],
      "metadata": {
        "id": "3u8fxiV31P9H"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def preprocess_function(examples):\n",
        "#     inputs = [prefix + doc for doc in examples[\"document\"]] # add summarize to documents\n",
        "#     model_inputs = tokenizer(inputs, max_length=1024, truncation=True) # tokenize inputs\n",
        "#     labels = tokenizer(text_target=examples[\"summary\"], max_length=128, truncation=True) # tokenize outputs\n",
        "\n",
        "#     model_inputs[\"labels\"] = labels[\"input_ids\"] # match up the text and summary. Specific to this application\n",
        "#     return model_inputs"
      ],
      "metadata": {
        "id": "MXwg-kpbIdUv"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenized_dataset = dataset.map(preprocess_function, batched=True)"
      ],
      "metadata": {
        "id": "JXxZ8UPs1PyZ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import DataCollatorForSeq2Seq\n",
        "# # data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint) # code when using Pytorch\n",
        "# data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint, return_tensors=\"tf\")"
      ],
      "metadata": {
        "id": "WaTnpmMp1Pl_"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oamncjUCBW1H"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Define Pipeline Evaluation Metrics**"
      ],
      "metadata": {
        "id": "-a3JOvUG1y0R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rouge = evaluate.load(\"rouge\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "\n",
        "    return {k: round(v, 4) for k, v in result.items()}"
      ],
      "metadata": {
        "id": "B71CACwW1zZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Define Model**"
      ],
      "metadata": {
        "id": "8me2ivgUrx9f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers[torch]"
      ],
      "metadata": {
        "id": "MZD2slGHfVId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate -U"
      ],
      "metadata": {
        "id": "H5zdusRZhL5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "lBCvvfjX10iA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jGAR2On7PqUp"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "P0wjSZEsPrUJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Original training args:\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"multi_news_train_4000t_t5-small_summary_model\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    weight_decay=0.01, # regularization in optimizer\n",
        "    save_total_limit=3, # maximum number of versions to have saved\n",
        "    num_train_epochs=3,\n",
        "    predict_with_generate= True\n",
        ")"
      ],
      "metadata": {
        "id": "JaBOJOBd10Wo"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Attempted training args:\n",
        "# # unable to run without crashing session\n",
        "\n",
        "# training_args = Seq2SeqTrainingArguments(\n",
        "#     output_dir=\"multi_news_train_4000arg_t5-small_summary_model\",\n",
        "#     save_strategy=\"epoch\",  # Saves at the end of each epoch\n",
        "#     save_total_limit=5,  # Increase if you have enough storage and want more checkpoints\n",
        "#     evaluation_strategy=\"steps\",  # Change from 'epoch' to evaluate more frequently\n",
        "#     eval_steps=400,  # Number of steps to run evaluation\n",
        "#     learning_rate=2e-5,\n",
        "#     per_device_train_batch_size=8, # increase to provide more accurate estimate of gradient\n",
        "#     per_device_eval_batch_size=8,\n",
        "#     weight_decay=0.01, # regularization in optimizer\n",
        "#     num_train_epochs=3,\n",
        "#     predict_with_generate= True,\n",
        "#     lr_scheduler_type=\"linear\",\n",
        "#     warmup_steps= 200 # (4000 samples/8 samples per batch) * 5 epochs\n",
        "# )"
      ],
      "metadata": {
        "id": "JE914JPjP0Vp"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build trainer\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ],
      "metadata": {
        "id": "gOsJ4Qh-10JC"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trainer.train()\n",
        "#untruncated version\n",
        "# trainer.save_model('multi_news_train_800_t5-small_summary_model')"
      ],
      "metadata": {
        "id": "Rq1-OkYrjibj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trainer.train()\n",
        "#untruncated version\n",
        "# trainer.save_model('multi_news_train_4000_t5-small_summary_model')"
      ],
      "metadata": {
        "id": "ddkLIx2cWEQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trainer.train()\n",
        "#truncated version\n",
        "# trainer.save_model('multi_news_train_4000t_t5-small_summary_model')"
      ],
      "metadata": {
        "id": "G5geaERsYuc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # attempted version\n",
        "# trainer.train()\n",
        "# trainer.save_model('multi_news_train_4000arg_t5-small_summary_model')"
      ],
      "metadata": {
        "id": "_GTOsV7U-VZl",
        "outputId": "7a257fda-c5fa-4a53-f7d3-482789085ed7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [   4/1500 01:33 < 19:29:51, 0.02 it/s, Epoch 0.01/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Login to HuggingFace**"
      ],
      "metadata": {
        "id": "BVbci2QGYmDZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "4TPUS6jHKsEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.push_to_hub('multi_news_train_4000arg_t5-small_summary_model')"
      ],
      "metadata": {
        "id": "rZNUXG2FvuNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document = ds_full['test'][0]['document']\n",
        "test_summary_example = ds_full['test'][0]['summary']\n",
        "test_summary_example"
      ],
      "metadata": {
        "id": "fT3i-xZEjkAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"multi_news_train_4000arg_t5-small_summary_model\", local_files_only=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"multi_news_train_4000arg_t5-small_summary_model\", local_files_only=True)\n",
        "\n",
        "# Tokenize the input text\n",
        "inputs = tokenizer(document, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "\n",
        "# Generate summary\n",
        "summary_ids = model.generate(inputs[\"input_ids\"], num_beams=4, min_length=None, max_length= 500, early_stopping=True)\n",
        "\n",
        "# Decode the summary\n",
        "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# Print the summary\n",
        "summary"
      ],
      "metadata": {
        "id": "sJl--6xRtdgR"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}