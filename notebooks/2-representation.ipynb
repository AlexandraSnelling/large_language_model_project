{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9oZAvi9EoHjB"
      },
      "outputs": [],
      "source": [
        "import sklearn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import TfidfVectorizer and CountVectorizer from sklearn\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
      ],
      "metadata": {
        "id": "5Mvru28QoM-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# processed_ds (from preprocessing notebook)\n",
        "dataset = processed_ds\n",
        "documents = dataset.data"
      ],
      "metadata": {
        "id": "qaP9N2xpoOOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(documents)\n",
        "documents[0]"
      ],
      "metadata": {
        "id": "RtVRRHFOoPPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TF-IDF**"
      ],
      "metadata": {
        "id": "8CUi8dDhqrYW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set number of features (output vocabulary)\n",
        "no_features = 100"
      ],
      "metadata": {
        "id": "0Qk3rrMnoPaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Assuming 'no_features' is a variable you've defined elsewhere with the desired number of features\n",
        "no_features = 1000  # Example value; adjust as needed\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(max_df=0.95,  # documents with a term frequency higher than this will be ignored\n",
        "                                   min_df=2,     # terms must appear in at least 2 documents to be considered\n",
        "                                   max_features=no_features,  # the size of the output vocabulary\n",
        "                                   stop_words='english')  # remove English stop words\n",
        "\n",
        "# To use the vectorizer, you need to provide it with a dataset to fit and transform\n",
        "# Example:\n",
        "# dataset = [\"document one text\", \"document two text\", ...]\n",
        "# tfidf_matrix = tfidf_vectorizer.fit_transform(dataset)"
      ],
      "metadata": {
        "id": "k6u4eURqqVQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fit and transform tfidf_vectorizer with dataset\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
        "# This tfidf_matrix can then be used as input for various machine learning models."
      ],
      "metadata": {
        "id": "IVE5K_QdqU2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inspect features\n",
        "tfidf_vectorizer.get_feature_names_out()"
      ],
      "metadata": {
        "id": "BAUmeDV6qUVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **NMF**"
      ],
      "metadata": {
        "id": "17CtXTwxsePX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "no_topics = 100"
      ],
      "metadata": {
        "id": "NNQeLOw5rgqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of topics/components\n",
        "n_components = no_topics\n",
        "\n",
        "# Instantiate and fit_transform data with NMF\n",
        "nmf = NMF(n_components=n_components, random_state=1, init='nndsvd')\n",
        "W = nmf.fit_transform(tfidf_matrix)  # W matrix contains the document-topic distributions\n",
        "H = nmf.components_  # H matrix contains the topic-term distributions\n",
        "\n",
        "# Now, W and H can be used for further analysis, like identifying dominant topics for documents"
      ],
      "metadata": {
        "id": "0gCK9SEHrgWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(H)"
      ],
      "metadata": {
        "id": "RXV_dM_Drf06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'nmf' is your fitted NMF model from the previous example\n",
        "# and 'tfidf_vectorizer' is the TfidfVectorizer you used to transform your documents\n",
        "\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "for topic_idx, topic in enumerate(nmf.components_):\n",
        "    print(f\"Topic #{topic_idx + 1}:\")\n",
        "\n",
        "    # Sort the weights in the topic and get the indices of the top 10 features\n",
        "    top_feature_indices = topic.argsort()[-10:][::-1]\n",
        "\n",
        "    # Map the indices to actual words\n",
        "    top_features = [feature_names[i] for i in top_feature_indices]\n",
        "    print(\" \".join(top_features))"
      ],
      "metadata": {
        "id": "yVH6rwNCrfFu"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}