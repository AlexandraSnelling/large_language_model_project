{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **Load Dataset**"
      ],
      "metadata": {
        "id": "xAvF3Q_Xnhbf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install datasets"
      ],
      "metadata": {
        "id": "v84iZoB1ncyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "_MAOPMdLngi8"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = load_dataset('multi_news')"
      ],
      "metadata": {
        "id": "gU7Ksb-jngSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Convert Dataset to Dataframe**"
      ],
      "metadata": {
        "id": "Bkgsq51mnyqL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert datasets to pandas dataframes\n",
        "import pandas as pd\n",
        "ds_train = pd.DataFrame(ds['train'])\n",
        "ds_test = pd.DataFrame(ds['test'])"
      ],
      "metadata": {
        "id": "WHgVizqkngBM"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Remove Punctuation**"
      ],
      "metadata": {
        "id": "00bscuo7oBUl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "# define funtion to remove punctuation.\n",
        "def remove_punctuation(review):\n",
        "    review = review.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "    return review"
      ],
      "metadata": {
        "id": "H2SvSTB8oEyG"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_train['document'] = ds_train['document'].apply(lambda x: remove_punctuation(x))\n",
        "ds_train['summary'] = ds_train['summary'].apply(lambda x: remove_punctuation(x))"
      ],
      "metadata": {
        "id": "3HU24SqioEWu"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_test['document'] = ds_test['document'].apply(lambda x: remove_punctuation(x))\n",
        "ds_test['summary'] = ds_test['summary'].apply(lambda x: remove_punctuation(x))"
      ],
      "metadata": {
        "id": "YoNuq-rCoSxU"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Convert Dataframe to Dataset**"
      ],
      "metadata": {
        "id": "qRByqR55oGZN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert dataframes to datasets\n",
        "from datasets import Dataset, DatasetDict\n",
        "# assign the splits\n",
        "train = Dataset.from_pandas(ds_train)\n",
        "test = Dataset.from_pandas(ds_test)\n",
        "# reconstruct both datasets into a Dataset Dict object\n",
        "processed_ds = DatasetDict(\n",
        "    {\n",
        "        'train': train,\n",
        "        'test': test\n",
        "    }\n",
        ")\n",
        "# view the resulting dataset dict object\n",
        "processed_ds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7AK6ckDpoHIr",
        "outputId": "f6ca1781-81cb-4703-8217-522634c85818"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['document', 'summary'],\n",
              "        num_rows: 44972\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['document', 'summary'],\n",
              "        num_rows: 5622\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Import Dependencies:**"
      ],
      "metadata": {
        "id": "NNnPyQlLo4NI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "9oZAvi9EoHjB"
      },
      "outputs": [],
      "source": [
        "import sklearn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import TfidfVectorizer from sklearn\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
      ],
      "metadata": {
        "id": "5Mvru28QoM-1"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = processed_ds.data"
      ],
      "metadata": {
        "id": "qaP9N2xpoOOS"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(processed_ds))\n",
        "print(type(ds_dict))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23L7MC5Mpt-4",
        "outputId": "778396ad-272e-4702-bb1c-2e214fc2a138"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'datasets.dataset_dict.DatasetDict'>\n",
            "<class 'dict'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(ds_dict['train']))\n",
        "print(type(ds_dict['train']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-GmowuZQqDpt",
        "outputId": "f580ad3a-3289-46e9-b679-bccd05d07379"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "44972\n",
            "<class 'datasets.table.InMemoryTable'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(documents)\n",
        "documents.keys()"
      ],
      "metadata": {
        "id": "RtVRRHFOoPPx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e81bfea-157c-4208-ea6d-9ee6f34bae57"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['train', 'test'])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TF-IDF**"
      ],
      "metadata": {
        "id": "8CUi8dDhqrYW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set number of features (output vocabulary)\n",
        "no_features = 100"
      ],
      "metadata": {
        "id": "0Qk3rrMnoPaW"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Assuming 'no_features' is a variable you've defined elsewhere with the desired number of features\n",
        "no_features = 100  # Example value; adjust as needed\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer( #max_df=0.95,  # documents with a term frequency higher than this will be ignored\n",
        "                                   #min_df=2,     # terms must appear in at least 2 documents to be considered\n",
        "                                   max_features=no_features,  # the size of the output vocabulary\n",
        "                                   stop_words='english')  # remove English stop words\n",
        "\n",
        "# To use the vectorizer, you need to provide it with a dataset to fit and transform\n",
        "# Example:\n",
        "# dataset = [\"document one text\", \"document two text\", ...]\n",
        "# tfidf_matrix = tfidf_vectorizer.fit_transform(dataset)"
      ],
      "metadata": {
        "id": "k6u4eURqqVQJ"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fit and transform tfidf_vectorizer with dataset\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(processed_ds)\n",
        "# This tfidf_matrix can then be used as input for various machine learning models."
      ],
      "metadata": {
        "id": "IVE5K_QdqU2D"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inspect features\n",
        "tfidf_vectorizer.get_feature_names_out()"
      ],
      "metadata": {
        "id": "BAUmeDV6qUVw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f646542-7449-4347-c152-b3d23b4e96ea"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['test', 'train'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **NMF**"
      ],
      "metadata": {
        "id": "17CtXTwxsePX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "no_topics = 100"
      ],
      "metadata": {
        "id": "NNQeLOw5rgqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of topics/components\n",
        "n_components = no_topics\n",
        "\n",
        "# Instantiate and fit_transform data with NMF\n",
        "nmf = NMF(n_components=n_components, random_state=1, init='nndsvd')\n",
        "W = nmf.fit_transform(tfidf_matrix)  # W matrix contains the document-topic distributions\n",
        "H = nmf.components_  # H matrix contains the topic-term distributions\n",
        "\n",
        "# Now, W and H can be used for further analysis, like identifying dominant topics for documents"
      ],
      "metadata": {
        "id": "0gCK9SEHrgWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(H)"
      ],
      "metadata": {
        "id": "RXV_dM_Drf06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'nmf' is your fitted NMF model from the previous example\n",
        "# and 'tfidf_vectorizer' is the TfidfVectorizer you used to transform your documents\n",
        "\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "for topic_idx, topic in enumerate(nmf.components_):\n",
        "    print(f\"Topic #{topic_idx + 1}:\")\n",
        "\n",
        "    # Sort the weights in the topic and get the indices of the top 10 features\n",
        "    top_feature_indices = topic.argsort()[-10:][::-1]\n",
        "\n",
        "    # Map the indices to actual words\n",
        "    top_features = [feature_names[i] for i in top_feature_indices]\n",
        "    print(\" \".join(top_features))"
      ],
      "metadata": {
        "id": "yVH6rwNCrfFu"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}